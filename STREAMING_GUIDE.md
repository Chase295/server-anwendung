# ğŸ”¥ Streaming Guide - Echtzeit AI-Antworten

Komplette Anleitung fÃ¼r das **Streaming-Feature** des IoT & Voice Orchestrators mit Flowise AI.

## ğŸ“– Inhaltsverzeichnis

1. [Was ist Streaming?](#was-ist-streaming)
2. [Warum Streaming?](#warum-streaming)
3. [Wie funktioniert es?](#wie-funktioniert-es)
4. [Konfiguration](#konfiguration)
5. [Technische Details](#technische-details)
6. [Troubleshooting](#troubleshooting)

---

## Was ist Streaming?

**Streaming** bedeutet, dass die AI-Antwort **Wort fÃ¼r Wort** (Token fÃ¼r Token) gesendet wird, wÃ¤hrend sie generiert wird - Ã¤hnlich wie bei ChatGPT.

### Vorher (ohne Streaming):
```
Benutzer: "ErzÃ¤hl mir eine Geschichte"
[â³ 5-10 Sekunden Wartezeit...]
AI: "Es war einmal ein tapferer Ritter..."
```

### Jetzt (mit Streaming):
```
Benutzer: "ErzÃ¤hl mir eine Geschichte"
AI: Es [âš¡ sofort]
AI: war [âš¡ sofort]
AI: einmal [âš¡ sofort]
AI: ein [âš¡ sofort]
AI: tapferer [âš¡ sofort]
AI: Ritter... [âš¡ sofort]
```

---

## Warum Streaming?

### âœ… Vorteile:

1. **Schnellere Reaktion**
   - Erste Tokens kommen **sofort** (< 1 Sekunde)
   - Benutzer sieht sofort dass etwas passiert
   
2. **Bessere Benutzererfahrung**
   - Keine lange Wartezeit ohne Feedback
   - Live-Anzeige wie bei ChatGPT
   
3. **Effizientere Ausgabe**
   - Text kann bereits gesprochen werden wÃ¤hrend er noch generiert wird
   - Perfekt fÃ¼r Voice-Assistenten
   
4. **Niedriger Latenz**
   - Start-to-First-Token: ~0.5-1 Sekunde
   - Token-to-Token: ~0.1-0.2 Sekunden

### âŒ Ohne Streaming:

- Wartezeit: 5-30 Sekunden je nach Antwort-LÃ¤nge
- Keine RÃ¼ckmeldung wÃ¤hrend der Generierung
- Benutzer weiÃŸ nicht ob System noch arbeitet

---

## Wie funktioniert es?

### 1. **Architektur-Ãœberblick**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Benutzer  â”‚
â”‚  (WS-In)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ "Hallo AI!"
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Node    â”‚ â† Streaming: ON âœ…
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Sendet zu Flowise
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flowise   â”‚
â”‚   Server    â”‚ â†’ Generiert Token fÃ¼r Token
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Server-Sent Events (SSE)
       â”‚ event: token
       â”‚ data: "Hallo"
       â”‚
       â”‚ event: token
       â”‚ data: "!"
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Node    â”‚ â† EmpfÃ¤ngt jeden Token
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Erstellt USO fÃ¼r jeden Token
       â”‚ final: false, false, false, ... true
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WS-Out     â”‚
â”‚   Node      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Sendet jeden Token sofort
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ test-ws-    â”‚
â”‚  out.py     â”‚ â†’ Zeigt Text live
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Datenfluss im Detail**

#### Phase 1: Start
```json
{
  "event": "start",
  "data": ""
}
```

#### Phase 2: Tokens (mehrere)
```json
{
  "event": "token",
  "data": "Hello"
}
{
  "event": "token",
  "data": " "
}
{
  "event": "token",
  "data": "world"
}
```

#### Phase 3: Metadaten (optional)
```json
{
  "event": "metadata",
  "data": "{\"chatId\":\"abc\",\"messageId\":\"xyz\"}"
}
```

#### Phase 4: Ende
```json
{
  "event": "end",
  "data": ""
}
```

### 3. **USO-Format beim Streaming**

Die AI Node erstellt fÃ¼r **jeden Token** ein eigenes USO:

**Token 1-N (Streaming-Chunks):**
```json
{
  "header": {
    "id": "session-123",
    "type": "text",
    "sourceId": "ai_node",
    "timestamp": 1697123456789,
    "final": false,  // âš¡ WICHTIG: false = weitere Chunks kommen!
    "control": {
      "action": "ai_response",
      "data": {
        "model": "flowise",
        "event": "token",
        "chunkNumber": 1
      }
    }
  },
  "payload": "Hello"  // Nur dieses eine Wort/Token
}
```

**Finales USO (Abschluss):**
```json
{
  "header": {
    "id": "session-123",
    "type": "text",
    "sourceId": "ai_node",
    "timestamp": 1697123460000,
    "final": true,  // âš¡ WICHTIG: true = Stream beendet!
    "control": {
      "action": "ai_response",
      "data": {
        "model": "flowise",
        "streamingComplete": true,
        "totalChunks": 147,
        "totalLength": 523
      }
    }
  },
  "payload": "Hello world!"  // Kompletter Text (optional)
}
```

---

## Konfiguration

### 1. **AI Node Einstellungen**

Im Flow-Editor:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KI / Flowise Node              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                 â”‚
â”‚  Flowise-Server: [Dropdown â–¼]  â”‚
â”‚  âœ… Streaming aktivieren        â”‚  â† Standard: AN
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optionen:**
- âœ… **Streaming aktivieren (empfohlen)**
  - Tokens werden sofort gesendet
  - Beste User-Experience
  - Niedrigste Latenz
  
- âŒ **Streaming deaktivieren**
  - Wartet auf komplette Antwort
  - Nur empfohlen wenn Downstream-Node komplette Antwort benÃ¶tigt

### 2. **Flowise Server Konfiguration**

Dein Flowise-Server muss **nichts Spezielles** konfigurieren - Streaming wird automatisch aktiviert wenn die AI Node es anfordert.

**API-URL Format:**
```
http://localhost:3000/api/v1/prediction/your-chatflow-id
```

### 3. **WS-Out Node fÃ¼r Streaming**

Empfohlene Einstellungen:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebSocket Out Node             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Ziel-URL:                      â”‚
â”‚    ws://localhost:8084/endpoint â”‚
â”‚                                 â”‚
â”‚  Sende-Format:                  â”‚
â”‚    âšª Nur Content â† Empfohlen!  â”‚
â”‚    âšª Komplettes USO (JSON)     â”‚
â”‚                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**"Nur Content"** ist perfekt fÃ¼r Streaming weil:
- Nur der reine Text wird gesendet
- Kein JSON-Overhead
- Live-Display im Terminal

---

## Technische Details

### Server-Sent Events (SSE)

Flowise nutzt das **SSE-Protokoll** fÃ¼r Streaming:

```
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

event: start
data: 

event: token
data: Hello

event: token
data:  world

event: end
data: 
```

### FlowiseService Implementation

**Backend: `flowise.service.ts`**

```typescript
async sendToFlowiseStreaming(
  config: FlowiseConfig,
  question: string,
  sessionId: string | undefined,
  chunkCallback: (chunk: string, event: string) => void,
  metadataCallback?: (metadata: any) => void
): Promise<void>
```

**Wichtige Features:**
1. âœ… Parst Server-Sent Events
2. âœ… UnterstÃ¼tzt verschachteltes JSON-Format von Flowise
3. âœ… Callback fÃ¼r jeden Token
4. âœ… Sammelt Metadaten
5. âœ… Error-Handling

### Verschachteltes Event-Format

**Problem:** Flowise sendet Events manchmal verschachtelt:

```json
{
  "event": "message",
  "data": "{\"event\":\"token\",\"data\":\"Hello\"}"
}
```

**LÃ¶sung:** Der Parser entpackt automatisch:
```typescript
if (event === 'message') {
  const nested = JSON.parse(data);
  actualEvent = nested.event;  // "token"
  actualData = nested.data;    // "Hello"
}
```

### AI Node Implementation

**Backend: `ai.node.ts`**

```typescript
private async processStreaming(
  uso: UniversalStreamObject,
  text: string,
  emitter: EventEmitter
): Promise<void> {
  let fullText = '';
  let chunkCount = 0;

  await this.flowiseService.sendToFlowiseStreaming(
    this.flowiseConfig!,
    text,
    uso.header.id,
    (chunk: string, event: string) => {
      // FÃ¼r jeden Token:
      fullText += chunk;
      chunkCount++;
      
      // Erstelle USO mit final=false
      const chunkUso = this.createOutputUSO(
        uso,
        chunk,
        false,  // Nicht final!
        { event, chunkNumber: chunkCount }
      );
      
      this.emitOutput(emitter, chunkUso);
    }
  );
  
  // Am Ende: Finales USO mit final=true
  const finalUso = this.createOutputUSO(
    uso,
    fullText,
    true,
    { streamingComplete: true, totalChunks: chunkCount }
  );
  
  this.emitOutput(emitter, finalUso);
}
```

### test-ws-out.py Implementation

**Streaming-Erkennung:**

```python
# Session-Tracking
streaming_sessions = {}

# Chunk empfangen
if not is_final:
    if session_id not in streaming_sessions:
        # ERSTER CHUNK - Zeige Header
        print("ğŸ”¥ STREAMING gestartet!")
        print("AI Antwort: ", end='', flush=True)
        
        streaming_sessions[session_id] = {
            'chunks': [],
            'chunk_count': 0
        }
    
    # Live-Anzeige
    print(payload, end='', flush=True)
    streaming_sessions[session_id]['chunks'].append(payload)

# Finales Paket
else:
    if session_id in streaming_sessions:
        print("\nâœ“ STREAMING abgeschlossen!")
        full_text = ''.join(session['chunks'])
        print(f"GesamtlÃ¤nge: {len(full_text)} Zeichen")
```

---

## Test-Workflow

### Kompletter Test-Ablauf:

```bash
# 1. Backend starten
cd "Server_anwendung"
docker-compose up -d

# 2. Test-Server starten (Terminal 1)
./test-ws-out.sh

# 3. Sende Frage (Terminal 2)
./test-ws-in.sh
# Eingabe: "ErzÃ¤hl mir eine kurze Geschichte"

# 4. Beobachte Live-Streaming in Terminal 1! âœ¨
```

### Erwartete Ausgabe:

**Terminal 1 (test-ws-out.py):**

```
[14:50:12.123] ğŸ“© Nachricht #1 empfangen
  â†’ Client ID: 4392873296

  â†’ USO-Format erkannt!
    â€¢ USO-Typ: text
    â€¢ Source: ai_node_123
    â€¢ Session ID: abc-123-def...
    â€¢ Context:
      - person: Moritz Haslbeck
      - location: Schlafzimmer
      - time: 2025-10-21 14:50:12

  ğŸ”¥ STREAMING gestartet!

  AI Antwort: Once upon a time, in a small village nestled between rolling hills, there lived a curious young fox named Finn. Finn loved exploring the forest...

[14:50:18.456] ğŸ“© Final-Nachricht #78 empfangen
  âœ“ STREAMING abgeschlossen!
    â€¢ Chunks: 147
    â€¢ GesamtlÃ¤nge: 523 Zeichen
    â€¢ Server-Chunks: 147
    â€¢ Server-LÃ¤nge: 523 Zeichen
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Beobachtungen:**
- âœ… Text erscheint **sofort** Wort fÃ¼r Wort
- âœ… Keine VerzÃ¶gerung zwischen Tokens
- âœ… Komplette Statistik am Ende
- âœ… Context-Info nur **einmal** am Anfang

---

## Troubleshooting

### Problem: Keine Tokens ankommen

**Symptome:**
```
totalChunks: 0
totalLength: 0
Leerer Payload
```

**LÃ¶sung:**
1. Backend neu starten: `docker-compose restart backend`
2. PrÃ¼fe Flowise-Server ist erreichbar
3. PrÃ¼fe Backend-Logs: `docker-compose logs -f backend | grep Flowise`

### Problem: "Unknown Flowise event"

**Symptome:**
```
debug [FlowiseService] Unknown Flowise event { event: "message", data: "..." }
```

**Ursache:** Verschachteltes Event-Format (bereits behoben in v2.0)

**LÃ¶sung:** Backend-Version prÃ¼fen und aktualisieren

### Problem: Streaming funktioniert nicht

**Checkliste:**
- [ ] AI Node: "Streaming aktivieren" ist âœ… AN
- [ ] Flowise-Server ist erreichbar
- [ ] Backend wurde neu kompiliert (`npm run build`)
- [ ] Backend wurde neu gestartet (`docker-compose restart backend`)
- [ ] test-ws-out.py verwendet neueste Version

### Problem: Text erscheint nicht live

**Symptome:** Alle Chunks auf einmal statt einzeln

**LÃ¶sung:** 
- `test-ws-out.py` verwendet jetzt `end='', flush=True` fÃ¼r Live-Display
- Stelle sicher du hast die neueste Version

### Problem: Doppelte Antworten

**Symptome:** Text erscheint zweimal

**Ursache:** Sowohl Chunks als auch finales Paket enthalten den Text

**LÃ¶sung:** Normal! Das finale Paket ist die Zusammenfassung.

---

## Performance-Metriken

### Typische Werte:

| Metrik | Wert | Beschreibung |
|--------|------|--------------|
| **Start-to-First-Token** | 0.5-1.5s | Zeit bis erster Token |
| **Token-to-Token** | 0.1-0.3s | Zeit zwischen Tokens |
| **Tokens pro Sekunde** | 10-30 | Generierungs-Geschwindigkeit |
| **Overhead pro Token** | ~400 bytes | USO-Header + Metadaten |
| **Netzwerk-Latenz** | < 10ms | Lokales Netzwerk |

### Vergleich:

```
Ohne Streaming:
â”œâ”€ Frage senden: 0.1s
â”œâ”€ AI denkt: 10s â³â³â³
â””â”€ Antwort empfangen: 0.1s
Total: 10.2s bis erste Anzeige

Mit Streaming:
â”œâ”€ Frage senden: 0.1s
â”œâ”€ Erster Token: 0.8s âš¡
â”œâ”€ Token 2-147: 0.2s pro Token âš¡âš¡âš¡
â””â”€ Fertig: 10.0s
Total: 0.9s bis erste Anzeige! ğŸš€
```

**11x schnellere Wahrnehmung!**

---

## Best Practices

### 1. **Verwende Streaming immer wenn mÃ¶glich**
   - Standard-Setting: âœ… AN
   - Nur deaktivieren wenn Downstream-Node komplette Antwort braucht

### 2. **WS-Out: "Nur Content" fÃ¼r Live-Display**
   - Perfekt fÃ¼r Text-Anzeige
   - Minimal Overhead
   - Beste Performance

### 3. **Error-Handling**
   - AI Node hat automatisches Retry
   - Reconnect bei Verbindungsverlust
   - Fehler werden geloggt

### 4. **Testing**
   - Verwende `test-ws-out.py` fÃ¼r Live-Ansicht
   - Debug-Events Panel im Frontend fÃ¼r Details
   - Backend-Logs fÃ¼r Troubleshooting

### 5. **Production-Ready**
   - Streaming ist stabil und getestet
   - Kein zusÃ¤tzlicher Setup nÃ¶tig
   - Funktioniert mit allen Flowise-Flows

---

## WeiterfÃ¼hrende Dokumentation

- **[TEST_SCRIPTS_README.md](TEST_SCRIPTS_README.md)** - Test-Scripts Verwendung
- **[NODES.md](NODES.md)** - Alle Node-Dokumentationen
- **[DEBUG_EVENTS_GUIDE.md](DEBUG_EVENTS_GUIDE.md)** - Debug-Events Panel
- **[CONTEXT_MANAGEMENT.md](CONTEXT_MANAGEMENT.md)** - Context-System

---

## Changelog

### Version 2.0 (Oktober 2025)
- âœ… Flowise Streaming implementiert
- âœ… Server-Sent Events (SSE) Parser
- âœ… Verschachteltes Event-Format unterstÃ¼tzt
- âœ… Live-Display in test-ws-out.py
- âœ… AI Node: enableStreaming Config-Option
- âœ… Automatisches Chunk-Tracking
- âœ… Health-Status fÃ¼r WS-Out Node
- âœ… Umfassende Dokumentation

---

**Erstellt:** Oktober 2025  
**Version:** 2.0  
**Autor:** AI Assistant & Moritz Haslbeck

