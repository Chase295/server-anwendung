# üî• Streaming Guide - Echtzeit AI-Antworten

Komplette Anleitung f√ºr das **Streaming-Feature** des IoT & Voice Orchestrators mit Flowise AI.

## üìñ Inhaltsverzeichnis

1. [Was ist Streaming?](#was-ist-streaming)
2. [Warum Streaming?](#warum-streaming)
3. [Wie funktioniert es?](#wie-funktioniert-es)
4. [Konfiguration](#konfiguration)
5. [Technische Details](#technische-details)
6. [Troubleshooting](#troubleshooting)

---

## Was ist Streaming?

**Streaming** bedeutet, dass die AI-Antwort **Wort f√ºr Wort** (Token f√ºr Token) gesendet wird, w√§hrend sie generiert wird - √§hnlich wie bei ChatGPT.

### Vorher (ohne Streaming):
```
Benutzer: "Erz√§hl mir eine Geschichte"
[‚è≥ 5-10 Sekunden Wartezeit...]
AI: "Es war einmal ein tapferer Ritter..."
```

### Jetzt (mit Streaming):
```
Benutzer: "Erz√§hl mir eine Geschichte"
AI: Es [‚ö° sofort]
AI: war [‚ö° sofort]
AI: einmal [‚ö° sofort]
AI: ein [‚ö° sofort]
AI: tapferer [‚ö° sofort]
AI: Ritter... [‚ö° sofort]
```

---

## Warum Streaming?

### ‚úÖ Vorteile:

1. **Schnellere Reaktion**
   - Erste Tokens kommen **sofort** (< 1 Sekunde)
   - Benutzer sieht sofort dass etwas passiert
   
2. **Bessere Benutzererfahrung**
   - Keine lange Wartezeit ohne Feedback
   - Live-Anzeige wie bei ChatGPT
   
3. **Effizientere Ausgabe**
   - Text kann bereits gesprochen werden w√§hrend er noch generiert wird
   - Perfekt f√ºr Voice-Assistenten
   
4. **Niedriger Latenz**
   - Start-to-First-Token: ~0.5-1 Sekunde
   - Token-to-Token: ~0.1-0.2 Sekunden

### ‚ùå Ohne Streaming:

- Wartezeit: 5-30 Sekunden je nach Antwort-L√§nge
- Keine R√ºckmeldung w√§hrend der Generierung
- Benutzer wei√ü nicht ob System noch arbeitet

---

## Wie funktioniert es?

### 1. **Architektur-√úberblick**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Benutzer  ‚îÇ
‚îÇ  (WS-In)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ "Hallo AI!"
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Node    ‚îÇ ‚Üê Streaming: ON ‚úÖ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Sendet zu Flowise
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Flowise   ‚îÇ
‚îÇ   Server    ‚îÇ ‚Üí Generiert Token f√ºr Token
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Server-Sent Events (SSE)
       ‚îÇ event: token
       ‚îÇ data: "Hallo"
       ‚îÇ
       ‚îÇ event: token
       ‚îÇ data: "!"
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Node    ‚îÇ ‚Üê Empf√§ngt jeden Token
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Erstellt USO f√ºr jeden Token
       ‚îÇ final: false, false, false, ... true
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  WS-Out     ‚îÇ
‚îÇ   Node      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Sendet jeden Token sofort
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ test-ws-    ‚îÇ
‚îÇ  out.py     ‚îÇ ‚Üí Zeigt Text live
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. **Datenfluss im Detail**

#### Phase 1: Start
```json
{
  "event": "start",
  "data": ""
}
```

#### Phase 2: Tokens (mehrere)
```json
{
  "event": "token",
  "data": "Hello"
}
{
  "event": "token",
  "data": " "
}
{
  "event": "token",
  "data": "world"
}
```

#### Phase 3: Metadaten (optional)
```json
{
  "event": "metadata",
  "data": "{\"chatId\":\"abc\",\"messageId\":\"xyz\"}"
}
```

#### Phase 4: Ende
```json
{
  "event": "end",
  "data": ""
}
```

### 3. **USO-Format beim Streaming**

Die AI Node erstellt f√ºr **jeden Token** ein eigenes USO:

**Token 1-N (Streaming-Chunks):**
```json
{
  "header": {
    "id": "session-123",
    "type": "text",
    "sourceId": "ai_node",
    "timestamp": 1697123456789,
    "final": false,  // ‚ö° WICHTIG: false = weitere Chunks kommen!
    "control": {
      "action": "ai_response",
      "data": {
        "model": "flowise",
        "event": "token",
        "chunkNumber": 1
      }
    }
  },
  "payload": "Hello"  // Nur dieses eine Wort/Token
}
```

**Finales USO (Abschluss):**
```json
{
  "header": {
    "id": "session-123",
    "type": "text",
    "sourceId": "ai_node",
    "timestamp": 1697123460000,
    "final": true,  // ‚ö° WICHTIG: true = Stream beendet!
    "control": {
      "action": "ai_response",
      "data": {
        "model": "flowise",
        "streamingComplete": true,
        "totalChunks": 147,
        "totalLength": 523
      }
    }
  },
  "payload": "Hello world!"  // Kompletter Text (optional)
}
```

---

## Konfiguration

### 1. **AI Node Einstellungen**

Im Flow-Editor:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  KI / Flowise Node              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                 ‚îÇ
‚îÇ  Flowise-Server: [Dropdown ‚ñº]  ‚îÇ
‚îÇ  ‚úÖ Streaming aktivieren        ‚îÇ  ‚Üê Standard: AN
‚îÇ                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Optionen:**
- ‚úÖ **Streaming aktivieren (empfohlen)**
  - Tokens werden sofort gesendet
  - Beste User-Experience
  - Niedrigste Latenz
  
- ‚ùå **Streaming deaktivieren**
  - Wartet auf komplette Antwort
  - Nur empfohlen wenn Downstream-Node komplette Antwort ben√∂tigt

### 2. **Flowise Server Konfiguration**

Dein Flowise-Server muss **nichts Spezielles** konfigurieren - Streaming wird automatisch aktiviert wenn die AI Node es anfordert.

**API-URL Format:**
```
http://localhost:3000/api/v1/prediction/your-chatflow-id
```

### 3. **WS-Out Node f√ºr Streaming**

Empfohlene Einstellungen:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  WebSocket Out Node             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Ziel-URL:                      ‚îÇ
‚îÇ    ws://localhost:8084/endpoint ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ  Sende-Format:                  ‚îÇ
‚îÇ    ‚ö™ Nur Content ‚Üê Empfohlen!  ‚îÇ
‚îÇ    ‚ö™ Komplettes USO (JSON)     ‚îÇ
‚îÇ                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**"Nur Content"** ist perfekt f√ºr Streaming weil:
- Nur der reine Text wird gesendet
- Kein JSON-Overhead
- Live-Display im Terminal

---

## Technische Details

### Server-Sent Events (SSE)

Flowise nutzt das **SSE-Protokoll** f√ºr Streaming:

```
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

event: start
data: 

event: token
data: Hello

event: token
data:  world

event: end
data: 
```

### FlowiseService Implementation

**Backend: `flowise.service.ts`**

```typescript
async sendToFlowiseStreaming(
  config: FlowiseConfig,
  question: string,
  sessionId: string | undefined,
  chunkCallback: (chunk: string, event: string) => void,
  metadataCallback?: (metadata: any) => void
): Promise<void>
```

**Wichtige Features:**
1. ‚úÖ Parst Server-Sent Events
2. ‚úÖ Unterst√ºtzt verschachteltes JSON-Format von Flowise
3. ‚úÖ Callback f√ºr jeden Token
4. ‚úÖ Sammelt Metadaten
5. ‚úÖ Error-Handling

### Verschachteltes Event-Format

**Problem:** Flowise sendet Events manchmal verschachtelt:

```json
{
  "event": "message",
  "data": "{\"event\":\"token\",\"data\":\"Hello\"}"
}
```

**L√∂sung:** Der Parser entpackt automatisch:
```typescript
if (event === 'message') {
  const nested = JSON.parse(data);
  actualEvent = nested.event;  // "token"
  actualData = nested.data;    // "Hello"
}
```

### AI Node Implementation

**Backend: `ai.node.ts`**

```typescript
private async processStreaming(
  uso: UniversalStreamObject,
  text: string,
  emitter: EventEmitter
): Promise<void> {
  let fullText = '';
  let chunkCount = 0;

  await this.flowiseService.sendToFlowiseStreaming(
    this.flowiseConfig!,
    text,
    uso.header.id,
    (chunk: string, event: string) => {
      // F√ºr jeden Token:
      fullText += chunk;
      chunkCount++;
      
      // Erstelle USO mit final=false
      const chunkUso = this.createOutputUSO(
        uso,
        chunk,
        false,  // Nicht final!
        { event, chunkNumber: chunkCount }
      );
      
      this.emitOutput(emitter, chunkUso);
    }
  );
  
  // Am Ende: Finales USO mit final=true
  const finalUso = this.createOutputUSO(
    uso,
    fullText,
    true,
    { streamingComplete: true, totalChunks: chunkCount }
  );
  
  this.emitOutput(emitter, finalUso);
}
```

### test-ws-out.py Implementation

**Streaming-Erkennung:**

```python
# Session-Tracking
streaming_sessions = {}

# Chunk empfangen
if not is_final:
    if session_id not in streaming_sessions:
        # ERSTER CHUNK - Zeige Header
        print("üî• STREAMING gestartet!")
        print("AI Antwort: ", end='', flush=True)
        
        streaming_sessions[session_id] = {
            'chunks': [],
            'chunk_count': 0
        }
    
    # Live-Anzeige
    print(payload, end='', flush=True)
    streaming_sessions[session_id]['chunks'].append(payload)

# Finales Paket
else:
    if session_id in streaming_sessions:
        print("\n‚úì STREAMING abgeschlossen!")
        full_text = ''.join(session['chunks'])
        print(f"Gesamtl√§nge: {len(full_text)} Zeichen")
```

---

## Test-Workflow

### Kompletter Test-Ablauf:

```bash
# 1. Backend starten
cd "Server_anwendung"
docker-compose up -d

# 2. Test-Server starten (Terminal 1)
./test-ws-out.sh

# 3. Sende Frage (Terminal 2)
./test-ws-in.sh
# Eingabe: "Erz√§hl mir eine kurze Geschichte"

# 4. Beobachte Live-Streaming in Terminal 1! ‚ú®
```

### Erwartete Ausgabe:

**Terminal 1 (test-ws-out.py):**

```
[14:50:12.123] üì© Nachricht #1 empfangen
  ‚Üí Client ID: 4392873296

  ‚Üí USO-Format erkannt!
    ‚Ä¢ USO-Typ: text
    ‚Ä¢ Source: ai_node_123
    ‚Ä¢ Session ID: abc-123-def...
    ‚Ä¢ Context:
      - person: Moritz Haslbeck
      - location: Schlafzimmer
      - time: 2025-10-21 14:50:12

  üî• STREAMING gestartet!

  AI Antwort: Once upon a time, in a small village nestled between rolling hills, there lived a curious young fox named Finn. Finn loved exploring the forest...

[14:50:18.456] üì© Final-Nachricht #78 empfangen
  ‚úì STREAMING abgeschlossen!
    ‚Ä¢ Chunks: 147
    ‚Ä¢ Gesamtl√§nge: 523 Zeichen
    ‚Ä¢ Server-Chunks: 147
    ‚Ä¢ Server-L√§nge: 523 Zeichen
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

**Beobachtungen:**
- ‚úÖ Text erscheint **sofort** Wort f√ºr Wort
- ‚úÖ Keine Verz√∂gerung zwischen Tokens
- ‚úÖ Komplette Statistik am Ende
- ‚úÖ Context-Info nur **einmal** am Anfang

---

## Troubleshooting

### Problem: Keine Tokens ankommen

**Symptome:**
```
totalChunks: 0
totalLength: 0
Leerer Payload
```

**L√∂sung:**
1. Backend neu starten: `docker-compose restart backend`
2. Pr√ºfe Flowise-Server ist erreichbar
3. Pr√ºfe Backend-Logs: `docker-compose logs -f backend | grep Flowise`

### Problem: "Unknown Flowise event"

**Symptome:**
```
debug [FlowiseService] Unknown Flowise event { event: "message", data: "..." }
```

**Ursache:** Verschachteltes Event-Format (bereits behoben in v2.0)

**L√∂sung:** Backend-Version pr√ºfen und aktualisieren

### Problem: Streaming funktioniert nicht

**Checkliste:**
- [ ] AI Node: "Streaming aktivieren" ist ‚úÖ AN
- [ ] Flowise-Server ist erreichbar
- [ ] Backend wurde neu kompiliert (`npm run build`)
- [ ] Backend wurde neu gestartet (`docker-compose restart backend`)
- [ ] test-ws-out.py verwendet neueste Version

### Problem: Text erscheint nicht live

**Symptome:** Alle Chunks auf einmal statt einzeln

**L√∂sung:** 
- `test-ws-out.py` verwendet jetzt `end='', flush=True` f√ºr Live-Display
- Stelle sicher du hast die neueste Version

### Problem: Doppelte Antworten

**Symptome:** Text erscheint zweimal

**Ursache:** Sowohl Chunks als auch finales Paket enthalten den Text

**L√∂sung:** Normal! Das finale Paket ist die Zusammenfassung.

---

## Performance-Metriken

### Typische Werte:

| Metrik | Wert | Beschreibung |
|--------|------|--------------|
| **Start-to-First-Token** | 0.5-1.5s | Zeit bis erster Token |
| **Token-to-Token** | 0.1-0.3s | Zeit zwischen Tokens |
| **Tokens pro Sekunde** | 10-30 | Generierungs-Geschwindigkeit |
| **Overhead pro Token** | ~400 bytes | USO-Header + Metadaten |
| **Netzwerk-Latenz** | < 10ms | Lokales Netzwerk |

### Vergleich:

```
Ohne Streaming:
‚îú‚îÄ Frage senden: 0.1s
‚îú‚îÄ AI denkt: 10s ‚è≥‚è≥‚è≥
‚îî‚îÄ Antwort empfangen: 0.1s
Total: 10.2s bis erste Anzeige

Mit Streaming:
‚îú‚îÄ Frage senden: 0.1s
‚îú‚îÄ Erster Token: 0.8s ‚ö°
‚îú‚îÄ Token 2-147: 0.2s pro Token ‚ö°‚ö°‚ö°
‚îî‚îÄ Fertig: 10.0s
Total: 0.9s bis erste Anzeige! üöÄ
```

**11x schnellere Wahrnehmung!**

---

## Best Practices

### 1. **Verwende Streaming immer wenn m√∂glich**
   - Standard-Setting: ‚úÖ AN
   - Nur deaktivieren wenn Downstream-Node komplette Antwort braucht

### 2. **WS-Out: "Nur Content" f√ºr Live-Display**
   - Perfekt f√ºr Text-Anzeige
   - Minimal Overhead
   - Beste Performance

### 3. **Error-Handling**
   - AI Node hat automatisches Retry
   - Reconnect bei Verbindungsverlust
   - Fehler werden geloggt

### 4. **Testing**
   - Verwende `test-ws-out.py` f√ºr Live-Ansicht
   - Debug-Events Panel im Frontend f√ºr Details
   - Backend-Logs f√ºr Troubleshooting

### 5. **Production-Ready**
   - Streaming ist stabil und getestet
   - Kein zus√§tzlicher Setup n√∂tig
   - Funktioniert mit allen Flowise-Flows

---

## Weiterf√ºhrende Dokumentation

- **[TEST_SCRIPTS_README.md](TEST_SCRIPTS_README.md)** - Test-Scripts Verwendung
- **[NODES.md](NODES.md)** - Alle Node-Dokumentationen
- **[DEBUG_EVENTS_GUIDE.md](DEBUG_EVENTS_GUIDE.md)** - Debug-Events Panel
- **[CONTEXT_MANAGEMENT.md](CONTEXT_MANAGEMENT.md)** - Context-System

---

## Changelog

### Version 2.0 (Oktober 2025)
- ‚úÖ Flowise Streaming implementiert
- ‚úÖ Server-Sent Events (SSE) Parser
- ‚úÖ Verschachteltes Event-Format unterst√ºtzt
- ‚úÖ Live-Display in test-ws-out.py
- ‚úÖ AI Node: enableStreaming Config-Option
- ‚úÖ Automatisches Chunk-Tracking
- ‚úÖ Health-Status f√ºr WS-Out Node
- ‚úÖ Umfassende Dokumentation

---

**Erstellt:** Oktober 2025  
**Version:** 2.0  
**Autor:** AI Assistant & Moritz Haslbeck

